""" Implements Probabilistic Neural Networks using PyTorch
    The output of a PNN defines a mean and a variance of a Gaussian.
    Then, a sample is generated by sampling from the distribution
    defined by this mean and variance.

    See:
    https://arxiv.org/abs/1612.01474
"""
# Import necessary libraries
import numpy as np 
import random 
import torch 

class Model():
    """ Contains PyTorch model """
    def __init__(self, seed):
        torch.manual_seed(seed)
        self.input_dim = 1
        self.hidden_units = 16
        self.output_dim = 2

        device = torch.device('cpu')
        # Define the model [1 x h x h x 2]
        self.model = torch.nn.Sequential(
            torch.nn.Linear(self.input_dim, self.hidden_units, bias=True),
            torch.nn.ReLU(),
            torch.nn.Linear(self.hidden_units, self.hidden_units, bias=True),
            torch.nn.ReLU(),
            torch.nn.Linear(self.hidden_units, self.hidden_units, bias=True),
            torch.nn.ReLU(),
            torch.nn.Linear(self.hidden_units, self.output_dim, bias=True),
        ).to(device)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)
        # self.optimizer = torch.optim.SGD(self.model.parameters(), lr=1e-2, momentum=0.5)
        self.losses = []

        self.max_var = 0
        self.min_var = float('inf')

    def softplus(self, x):
        """ Positivity constraint """
        softplus = torch.log(1+torch.exp(x))
        # Avoid infinities due to taking the exponent
        softplus = torch.where(softplus==float('inf'), x, softplus)
        return softplus

    def adjust_learning_rate(self):
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = param_group['lr']*0.999

    def NLL(self, means, var, truth):
        """ Compute the Negative Log Likelihood """
        diff = torch.sub(truth, means)
        var = self.softplus(var)
        # Check min and max variance
        for v in var:
            if v == float('inf'):
                raise ValueError('Infinite variance')
            if v > self.max_var:
                self.max_var = v 
            if v < self.min_var:
                self.min_var = v
        loss = torch.mean(torch.div(diff**2, var))
        loss += torch.mean(torch.log(var))
        return loss 

    def forward(self, inputs):
        """ Forward pass for given inputs """
        x = torch.from_numpy(inputs).unsqueeze(-1).float()
        # Compute output of the trained model
        model_out = self.model(x)
        mean, var = torch.split(model_out, 1, dim=1)
        # Bound the variance
        var = self.softplus(var)
        # TRICK -- not added until now, actually increases overall variance
        # Avoids exploding std
        # logvar = torch.log(var)
        # logvar = self.max_var - self.softplus(self.max_var-logvar)
        # logvar = self.min_var + self.softplus(logvar-self.min_var)
        # var = torch.exp(logvar)
        return mean.detach().numpy(), var.detach().numpy()

    def step(self, inputs, labels):
        """ Execute a single gradient step using the given samples """
        # Convert samples to useable tensors
        # - need to unsqueeze since PyTorch only takes mini-batches
        #   we need to add an additional dimension
        # - for some reason we also need to call .float()
        #   even though x and y are already of type torch.float64
        x = torch.from_numpy(inputs).unsqueeze(-1).float()
        y = torch.from_numpy(labels).unsqueeze(-1).float()
        # Compute the output of the network
        model_out = self.model(x)
        means, var = torch.split(model_out, 1, dim=1)
        # Compute the NNL loss
        nll = self.NLL(means, var, y)
        self.losses.append(nll)

        # Do the backward pass through the network
        self.optimizer.zero_grad()
        nll.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5)
        self.optimizer.step()

        self.adjust_learning_rate()
